# -*- coding: utf-8 -*-
"""OLUWAKEMI_SHOJUPE_@00747127

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u7Se2g7KLcKUkw6ZSIp0AFDrQV9LEIJD
"""

## #install libraries
# !pip install tensorflow~=2.13.0
# !pip install keras~=2.13.0

!pip install keras-tcn
!pip install shap
!pip install statsmodels

"""##### IMPORT LIBRARIES"""

import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from statsmodels.tsa.seasonal import seasonal_decompose
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller
import seaborn as sns
import xgboost as xgb
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from statsmodels.tsa.arima.model import ARIMA

# Load the datasets
file_paths = {
    "2019": "/content/drive/MyDrive/energy/historic_demand_year_2019.csv",
    "2020": "/content/drive/MyDrive/energy/historic_demand_year_2020.csv",
    "2021": "/content/drive/MyDrive/energy/historic_demand_year_2021.csv",
    "2022": "/content/drive/MyDrive/energy/historic_demand_year_2022.csv",
    "2023": "/content/drive/MyDrive/energy/historic_demand_year_2023.csv"
}

# # Read the CSV files
# dfs =  {year: pd.read_csv(path) for year, path in file_paths.items()}

# Create a dictionary to store the dataframes
dataframes = {}
for year, path in file_paths.items():
    df = pd.read_csv(path)
    df.columns = df.columns.str.upper()  # Convert column headers to uppercase
    dataframes[year] = df

# Combine the data into a single dataframe
combined_df = pd.concat(dataframes)

# Display the first few rows of the combined dataset
combined_df.head()

# Attempt to parse dates
combined_df['DATETIME'] = pd.to_datetime(combined_df['SETTLEMENT_DATE'], errors='coerce', format='%d-%b-%Y')

# Identify and parse remaining dates (if any)
remaining_dates = combined_df[combined_df['DATETIME'].isna()]['SETTLEMENT_DATE']


# Assuming other date format is '%Y-%m-%d'
combined_df.loc[combined_df['DATETIME'].isna(), 'DATETIME'] = pd.to_datetime(remaining_dates, format='%Y-%m-%d')

# Combine with SETTLEMENT_PERIOD to create the full datetime index
combined_df['DATETIME'] += pd.to_timedelta(combined_df['SETTLEMENT_PERIOD'] * 30, unit='m')

# Filter out entries from the year 2024
combined_df = combined_df[combined_df['DATETIME'].dt.year != 2024]

# Set datetime as index
combined_df.set_index('DATETIME', inplace=True)

# Resample ND to daily frequency by summing the demand values
hourly_demand = combined_df['ND'].resample('H').mean()

# Display the head of daily_demand to verify
hourly_demand.head()

"""#### EXPLORATORY DATA ANALYSIS"""

# Summary statistics
summary_stats = hourly_demand.describe()
summary_stats

### Yearly time series plot

# Plot the data for each year separately
# Reset the index to access the 'year' attribute
combined_df.reset_index(inplace=True)

# Get the unique years
years = combined_df['DATETIME'].dt.year.unique()

# Reindex the dataframe back to datetime index for plotting
combined_df.set_index('DATETIME', inplace=True)

for year in years:
    plt.figure(figsize=(25, 6))
    yearly_data = hourly_demand[hourly_demand.index.year == year]
    plt.plot(yearly_data.index, yearly_data, label=f'Hourly Demand {year}')
    plt.title(f'Hourly Energy Demand in {year}')
    plt.xlabel('Date')
    plt.ylabel('Energy Demand')
    plt.legend()
    plt.show()

### Treend of ENERGY DEMAND(NATIONAL)


## Perform time series decomposition to extract the trend component
result = seasonal_decompose(hourly_demand, model='additive', period=24*365)  # Assuming hourly data with daily seasonality

# Extract the trend component
trend = result.trend.dropna()

# Plotting
plt.figure(figsize=(15, 6))
plt.plot(trend, label='Trend', color='blue')
plt.title('Trend of Energy Demand')
plt.xlabel('Time')
plt.ylabel('Energy Demand (MW)')
plt.legend(loc='best')
plt.show()

# Histogram of the demand
plt.figure(figsize=(14, 7))
hourly_demand.hist(bins=50)
plt.title('Distribution of Hourly ND')
plt.xlabel('Demand')
plt.ylabel('Frequency')
plt.show()

# Calculate the weekly average trend
weekly_trend = hourly_demand.groupby([hourly_demand.index.dayofweek, hourly_demand.index.hour]).mean().unstack(level=0)

# Plot the weekly trend
plt.figure(figsize=(14, 8))
for day in range(7):
    plt.plot(weekly_trend.index, weekly_trend[day], label=f'Day {day}')

plt.title('Average Hourly Energy Demand - Weekly Trend')
plt.xlabel('Hour of the Day')
plt.ylabel('Average Energy Demand')
plt.legend(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], loc='upper left')
plt.grid(True)
plt.show()

# Calculate the monthly average trend
monthly_trend = hourly_demand.groupby([hourly_demand.index.month, hourly_demand.index.hour]).mean().unstack(level=0)

# Plot the monthly trend
plt.figure(figsize=(14, 8))
for month in range(1, 13):
    plt.plot(monthly_trend.index, monthly_trend[month], label=f'Month {month}')

plt.title('Average Hourly Energy Demand - Monthly Trend')
plt.xlabel('Hour of the Day')
plt.ylabel('Average Energy Demand')
plt.legend([f'January', f'February', f'March', f'April', f'May', f'June', f'July', f'August', f'September', f'October', f'November', f'December'], loc='upper left')
plt.grid(True)
plt.show()

# Decompose the time series, use the values of the series directly
decomposition = seasonal_decompose(hourly_demand.dropna(), model='additive', period=24*365)

# Plot the decomposition
plt.figure(figsize=(14, 10))
plt.subplot(411)
plt.plot(decomposition.observed, label='Observed')
plt.legend(loc='upper left')
plt.subplot(412)
plt.plot(decomposition.trend, label='Trend')
plt.legend(loc='upper left')
plt.subplot(413)
plt.plot(decomposition.seasonal, label='Seasonal')
plt.legend(loc='upper left')
plt.subplot(414)
plt.plot(decomposition.resid, label='Residual')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

### SUBSET OF A YEAR

# Assuming 'hourly_demand' is a Series with a DatetimeIndex
subset_hourly_demand = hourly_demand[(hourly_demand.index >= '2023-01-01') &
                                     (hourly_demand.index < '2023-12-31')]

# Decompose the time series using the subset
decomposition = seasonal_decompose(subset_hourly_demand.dropna(),
                                   model='additive', period=24*7)  # Adjust period as needed

# Plot the decomposition
plt.figure(figsize=(14, 10))

plt.subplot(411)
plt.plot(decomposition.observed, label='Observed')
plt.legend(loc='upper left')

plt.subplot(412)
plt.plot(decomposition.trend, label='Trend')
plt.legend(loc='upper left')

plt.subplot(413)
plt.plot(decomposition.seasonal, label='Seasonal')
plt.legend(loc='upper left')

plt.subplot(414)
plt.plot(decomposition.resid, label='Residual')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

# Resample to hourly frequency by averaging the demand values
hourly_demand = combined_df['ND'].resample('H').mean().reset_index()

# Create features for hour, day of the week, and month
hourly_demand['HOUR'] = hourly_demand['DATETIME'].dt.hour
hourly_demand['DAY_OF_WEEK'] = hourly_demand['DATETIME'].dt.dayofweek
hourly_demand['MONTH'] = hourly_demand['DATETIME'].dt.month
hourly_demand['YEAR'] = hourly_demand['DATETIME'].dt.year

hourly_demand.head()

# Box plot by hour of the day
plt.figure(figsize=(25, 8))
sns.boxplot(x='HOUR', y='ND', data=hourly_demand)
plt.title('Box Plot of Hourly Energy Demand by Hour of the Day')
plt.xlabel('Hour of the Day')
plt.ylabel('Energy Demand')
plt.grid(True)
plt.show()

# Box plot by day of the week
plt.figure(figsize=(25, 8))
sns.boxplot(x='DAY_OF_WEEK', y='ND', data=hourly_demand)
plt.title('Box Plot of Hourly Energy Demand by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Energy Demand')
plt.xticks(ticks=range(7), labels=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.grid(True)
plt.show()

# Box plot by month of the year
plt.figure(figsize=(25, 8))
sns.boxplot(x='MONTH', y='ND', data=hourly_demand)
plt.title('Box Plot of Hourly Energy Demand by Month of the Year')
plt.xlabel('Month')
plt.ylabel('Energy Demand')
plt.xticks(ticks=range(0, 12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.grid(True)
plt.show()

# Box plot by year
plt.figure(figsize=(25, 7))
sns.boxplot(x='YEAR', y='ND', data=hourly_demand)
plt.title('Yearly Distribution of Energy Demand')
plt.xlabel('Year')
plt.ylabel('Energy Demand')
plt.show()

### CORRELATION MATRIX
# Drop the 'DATETIME' column if it's present
if 'DATETIME' in hourly_demand.columns:
    hourly_demand_numerical = hourly_demand.drop(columns=['DATETIME'])
else:
    hourly_demand_numerical = hourly_demand

# Compute the correlation matrix for the numerical features
correlation_matrix = hourly_demand_numerical.corr()

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)

# Set the title
plt.title('Correlation Heatmap of Features')

# Show the plot
plt.show()

plt.figure(figsize=(14, 7))
heatmap_data = hourly_demand.pivot_table(values='ND', index='HOUR', columns='MONTH')
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='.1f')
plt.title('Average Hourly Energy Demand by Month') # Changed title to reflect the data being visualized
plt.xlabel('Month')
plt.ylabel('Hour of Day')
plt.show()

#### ACF and PACF
# Create a 1x2 grid for the ACF and PACF plots
fig, axs = plt.subplots(1, 2, figsize=(15, 7))

# Autocorrelation plot
plot_acf(hourly_demand['ND'].dropna(), lags=50, ax=axs[0])
axs[0].set_title('Autocorrelation Function (ACF)')

# Partial Autocorrelation plot
plot_pacf(hourly_demand['ND'].dropna(), lags=50, ax=axs[1])
axs[1].set_title('Partial Autocorrelation Function (PACF)')

plt.tight_layout()
plt.show()

#### STATIONARY CHECK
# Function to check stationarity
def check_stationarity(timeseries):
    result = adfuller(timeseries)
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    if result[1] < 0.05:
        print('Series is stationary')
    else:
        print('Series is not stationary')

# Check stationarity of the demand data
check_stationarity(hourly_demand['ND'])

# Save the forecast DataFrame to an Excel file
hourly_demand.to_excel('hourly_demand.xlsx', index=False)

"""#### TRAIN TEST SPLIT"""

# Prepare the data
data = hourly_demand[['DATETIME', 'ND']].rename(columns={'DATETIME': 'ds', 'ND': 'y'})

# Ensure that 'ds' column is of datetime type
data['ds'] = pd.to_datetime(data['ds'])

# Split the data into training and testing sets
split = int(0.8 * len(data))
train = data.iloc[:split]
test = data.iloc[split:]



# Define the start and end date for the week you want to focus on
start_date = pd.Timestamp('2023-11-01')
end_date = pd.Timestamp('2023-11-07')

# Define the start and end date for the week you want to focus on
start_datem = pd.Timestamp('2023-10-01')
end_datem = pd.Timestamp('2023-12-31')

"""#### MODELING

#### MACHINE LEARNING TECNIQUES

##### RANDOM FOREST
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Ensure the datetime column is correctly parsed
hourly_demand['DATETIME'] = pd.to_datetime(hourly_demand['DATETIME'])

# Prepare the data
data = hourly_demand[['DATETIME', 'ND']].rename(columns={'DATETIME': 'ds', 'ND': 'y'})
data['ds'] = pd.to_datetime(data['ds'])

# Set the datetime column as index
data.set_index('ds', inplace=True)

# Create lag features
data['lag_1'] = data['y'].shift(1)
data['lag_2'] = data['y'].shift(2)
data['lag_3'] = data['y'].shift(3)

# Create rolling mean features
data['rolling_mean_24'] = data['y'].rolling(window=24).mean()
data['rolling_mean_48'] = data['y'].rolling(window=48).mean()

# Create additional time-based features
data['HOUR'] = data.index.hour
data['DAY_OF_WEEK'] = data.index.dayofweek
data['MONTH'] = data.index.month
data['YEAR'] = data.index.year
data['IS_WEEKEND'] = data['DAY_OF_WEEK'].apply(lambda x: 1 if x >= 5 else 0)

# Drop missing values
data.dropna(inplace=True)

# Split the data into training and testing sets
split = int(0.8 * len(data))
train = data.iloc[:split]
test = data.iloc[split:]

# Define features and target for both train and test
train_features = train.drop(columns='y')
train_target = train['y']
test_features = test.drop(columns='y')
test_target = test['y']

from sklearn.preprocessing import MinMaxScaler

# Normalize the data
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(train_features)
X_test_scaled = scaler.transform(test_features)

# Initialize the Random Forest model
rf_model = RandomForestRegressor(random_state=42)

# Define the reduced parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100],  # Reduced options for faster computation
    'max_depth': [10, 20],      # Narrower range for faster tuning
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')

# Fit the grid search to the training data
grid_search.fit(X_train_scaled, train_target)

# Get the best parameters and model
best_rf_model = grid_search.best_estimator_
print(f'Best parameters found: {grid_search.best_params_}')

# Make predictions on the test data
rf_predictions = best_rf_model.predict(X_test_scaled)

# Calculate evaluation metrics
mae_rf = mean_absolute_error(test_target, rf_predictions)
rmse_rf = np.sqrt(mean_squared_error(test_target, rf_predictions))
mape_rf = np.mean(np.abs((test_target - rf_predictions) / test_target)) * 100

# Print evaluation metrics
print(f'Random Forest - MAE: {mae_rf}')
print(f'Random Forest - RMSE: {rmse_rf}')
print(f'Random Forest - MAPE: {mape_rf:.2f}%')

# Plot the results
plt.figure(figsize=(25, 8))
plt.plot(test_features.index, test_target, 'r-', label='Test Data')
plt.plot(test_features.index, rf_predictions, 'b-', label='Forecast (Random Forest)')
plt.title('Random Forest Regressor - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand')
plt.legend()
plt.grid(True)
plt.show()

# Filter the test set and forecasted values for the specific period
test_subset = test[(test.index >= start_date) & (test.index <= end_date)]

# Get the integer positions of the test subset in the original test set
indices = test.index.get_indexer(test_subset.index)

forecast_subset = pd.DataFrame({
    'forecasted_values': rf_predictions[indices], # Use integer indices to select from rf_predictions
}, index=test_subset.index)

# Plot the results for the specific period
fig = plt.figure(figsize=(25, 8))
plt.plot(test_subset.index, test_subset['y'], 'r--', label='Test Data')  # Dotted line for test data
plt.plot(forecast_subset.index, forecast_subset['forecasted_values'], 'b-', label='Forecast')  # Solid line for predictions
plt.title('Random Forest Regressor - Test Data vs Forecast for November 1-7, 2023')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Filter the test set and forecasted values for the specific period
test_subset = test[(test.index >= start_datem) & (test.index <= end_datem)]

# Get the integer positions of the test subset in the original test set
indices = test.index.get_indexer(test_subset.index)

forecast_subset = pd.DataFrame({
    'forecasted_values': rf_predictions[indices], # Use integer indices to select from rf_predictions
}, index=test_subset.index)

# Plot the results for the specific period
fig = plt.figure(figsize=(25, 8))
plt.plot(test_subset.index, test_subset['y'], 'r--', label='Test Data')  # Dotted line for test data
plt.plot(forecast_subset.index, forecast_subset['forecasted_values'], 'b-', label='Forecast')  # Solid line for predictions
plt.title('Random Forest Regressor - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()



import shap

# Feature importance with SHAP using a 10% sample of the data
sample_size = int(0.01 * X_train_scaled.shape[0])
X_sample = X_train_scaled[np.random.choice(X_train_scaled.shape[0], sample_size, replace=False)]
train_sample_features = train_features.iloc[np.random.choice(train_features.shape[0], sample_size, replace=False)]


explainer = shap.TreeExplainer(best_rf_model)
shap_values = explainer.shap_values(X_sample)

# Create a SHAP summary plot using the sample
shap.summary_plot(shap_values, train_sample_features)











"""### XGBOOST"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Ensure the datetime column is correctly parsed
hourly_demand['DATETIME'] = pd.to_datetime(hourly_demand['DATETIME'])

# Prepare the data
data = hourly_demand[['DATETIME', 'ND']].rename(columns={'DATETIME': 'ds', 'ND': 'y'})
data['ds'] = pd.to_datetime(data['ds'])

# Set the datetime column as index
data.set_index('ds', inplace=True)

# Create lag features
data['lag_1'] = data['y'].shift(1)
data['lag_2'] = data['y'].shift(2)
data['lag_3'] = data['y'].shift(3)

# Create rolling mean features
data['rolling_mean_24'] = data['y'].rolling(window=24).mean()


# Create additional time-based features
data['HOUR'] = data.index.hour
data['DAY_OF_WEEK'] = data.index.dayofweek
data['MONTH'] = data.index.month


# Drop missing values
data.dropna(inplace=True)

# Split the data into training and testing sets
split = int(0.8 * len(data))
train = data.iloc[:split]
test = data.iloc[split:]

# Define features and target for both train and test
train_features = train.drop(columns='y')
train_target = train['y']
test_features = test.drop(columns='y')
test_target = test['y']

from sklearn.preprocessing import MinMaxScaler

# Normalize the data
xgb_scaler = MinMaxScaler()
X_train_scaled = xgb_scaler.fit_transform(train_features)
X_test_scaled = xgb_scaler.transform(test_features)

# Initialize the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Define the reduced parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [25, 50],   # Reduced options for faster computation
    'max_depth': [5, 10],       # Narrower range for faster tuning
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize GridSearchCV with n_jobs=1 to avoid potential serialization issues
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           cv=3, n_jobs=1, verbose=2, scoring='neg_mean_absolute_error')

# Fit the grid search to the data
grid_search.fit(X_train_scaled, train_target)

# Get the best parameters
best_xgb_model = grid_search.best_estimator_
print(f'Best XGBoost parameters found: {grid_search.best_params_}')

# Make predictions on the test data
xgb_predictions = best_xgb_model.predict(X_test_scaled)

# Calculate evaluation metrics
mae_xgb = mean_absolute_error(test_target, xgb_predictions)
rmse_xgb = np.sqrt(mean_squared_error(test_target, xgb_predictions))
mape_xgb = np.mean(np.abs((test_target - xgb_predictions) / test_target)) * 100

print(f'XGBoost - MAE: {mae_xgb}')
print(f'XGBoost - RMSE: {rmse_xgb}')
print(f'XGBoost - MAPE: {mape_xgb:.2f}%')

# Plot the results for the full test set
plt.figure(figsize=(25, 8))
plt.plot(test.index, test_target, 'r-', label='Test Data')
plt.plot(test.index, xgb_predictions, 'b--', label='Forecast (XGBoost)')
plt.title('XGBoost Model - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand')
plt.legend()
plt.grid(True)
plt.show()

test_subset = test[(test.index >= start_datem) & (test.index <= end_datem)]

# Get integer index positions for the subset
index_positions = test.index.get_indexer(test_subset.index)

forecast_subset = pd.DataFrame({
    'forecasted_values': xgb_predictions[index_positions],  # Use integer positions to slice
}, index=test_subset.index)

plt.figure(figsize=(25, 8))
plt.plot(test_subset.index, test_subset['y'], 'r--', label='Test Data')  # Dotted line for actual test data
plt.plot(forecast_subset.index, forecast_subset['forecasted_values'], 'b-', label='Forecast (XGBoost)')
plt.title('XGBoost Model - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

test_subset = test[(test.index >= start_date) & (test.index <= end_date)]

# Get integer index positions for the subset
index_positions = test.index.get_indexer(test_subset.index)

forecast_subset = pd.DataFrame({
    'forecasted_values': xgb_predictions[index_positions],  # Use integer positions to slice
}, index=test_subset.index)

plt.figure(figsize=(25, 8))
plt.plot(test_subset.index, test_subset['y'], 'r--', label='Test Data')  # Dotted line for actual test data
plt.plot(forecast_subset.index, forecast_subset['forecasted_values'], 'b-', label='Forecast (XGBoost)')
plt.title('XGBoost Model - Test Data vs Forecast for November 2023')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()



"""AI TECHNIQUES"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

"""LSTM"""



# Ensure the datetime column is correctly parsed
hourly_demand['DATETIME'] = pd.to_datetime(hourly_demand['DATETIME'])

# Prepare the data
data = hourly_demand[['DATETIME', 'ND']].rename(columns={'DATETIME': 'ds', 'ND': 'y'})

# Ensure that 'ds' column is of datetime type
data['ds'] = pd.to_datetime(data['ds'])

# Set the datetime column as index
data.set_index('ds', inplace=True)

# Scale the target variable
scaler = MinMaxScaler()
data['y'] = scaler.fit_transform(data[['y']])

# Create lag features for LSTM
def create_lag_features(data, n_lags):
    X, y = [], []
    for i in range(n_lags, len(data)):
        X.append(data[i-n_lags:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

# Define number of lags
n_lags = 24

# Create features and target for LSTM
X, y = create_lag_features(data.values, n_lags)

# Split the data into training and testing sets
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Reshape the input to be 3D [samples, timesteps, features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Build the LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
lstm_model.add(LSTM(50, return_sequences=False))
lstm_model.add(Dense(25))
lstm_model.add(Dense(1))

# Compile the model
lstm_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model with reduced epochs and increased batch size
lstm_model.fit(X_train, y_train, batch_size=32, epochs=20)

# Make predictions
lstm_predictions = lstm_model.predict(X_test)
lstm_predictions = scaler.inverse_transform(lstm_predictions.reshape(-1, 1))

# Inverse transform the test target as well
y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

# Calculate evaluation metrics
mae_lstm = mean_absolute_error(y_test, lstm_predictions)
rmse_lstm = np.sqrt(mean_squared_error(y_test, lstm_predictions))
mape_lstm = np.mean(np.abs((y_test - lstm_predictions) / y_test)) * 100

# Print evaluation metrics
print(f'LSTM - MAE: {mae_lstm}')
print(f'LSTM - RMSE: {rmse_lstm}')
print(f'LSTM - MAPE: {mape_lstm:.2f}%')

# Create a DataFrame to plot the results
results = pd.DataFrame({
    'Actual': y_test.flatten(),
    'Predicted': lstm_predictions.flatten()
}, index=data.index[-len(y_test):])

# Plot the results for LSTM
fig = plt.figure(figsize=(25, 8))
plt.plot(results.index, results['Actual'], 'r.', label='Test Data')
plt.plot(results.index, results['Predicted'], 'b-', label='Forecast (LSTM)')
plt.title('LSTM - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand')
plt.legend()
plt.grid(True)
plt.show()

# Filter the results DataFrame to get the subset for the specific date range
subset_results = results[(results.index >= start_datem) & (results.index <= end_datem)]

# Plot the results for the specific date range
fig = plt.figure(figsize=(25, 8))
plt.plot(subset_results.index, subset_results['Actual'], 'r--', label='Test Data') # Dotted line for test data
plt.plot(subset_results.index, subset_results['Predicted'], 'b-', label='Forecast (LSTM)') # Solid line for forecast
plt.title('LSTM Model - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Filter the results DataFrame to get the subset for the specific date range
subset_results = results[(results.index >= start_date) & (results.index <= end_date)]

# Plot the results for the specific date range
fig = plt.figure(figsize=(25, 8))
plt.plot(subset_results.index, subset_results['Actual'], 'r--', label='Test Data') # Dotted line for test data
plt.plot(subset_results.index, subset_results['Predicted'], 'b-', label='Forecast (LSTM)') # Solid line for forecast
plt.title('LSTM Model - Test Data vs Forecast for November 1-7, 2023')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

"""TCN"""



import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tcn import TCN, tcn_full_summary
import matplotlib.pyplot as plt

# Ensure the datetime column is correctly parsed
hourly_demand['DATETIME'] = pd.to_datetime(hourly_demand['DATETIME'])

# Prepare the data
data = hourly_demand[['DATETIME', 'ND']].rename(columns={'DATETIME': 'ds', 'ND': 'y'})

# Ensure that 'ds' column is of datetime type
data['ds'] = pd.to_datetime(data['ds'])

# Set the datetime column as index
data.set_index('ds', inplace=True)

# Scale only the target variable
scaler = MinMaxScaler()
data['y'] = scaler.fit_transform(data[['y']])

# Split the data into training and testing sets
split = int(0.8 * len(data))
train = data.iloc[:split]
test = data.iloc[split:]

# Prepare the data for TCN
def create_sequences(data, sequence_length):
    xs = []
    ys = []
    for i in range(len(data)-sequence_length):
        x = data.iloc[i:(i+sequence_length)].values
        y = data.iloc[i+sequence_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Define sequence length
sequence_length = 24

# Create sequences for TCN
X_train, y_train = create_sequences(train, sequence_length)
X_test, y_test = create_sequences(test, sequence_length)

# Reshape the input to be 3D [samples, timesteps, features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Build the TCN model
tcn_model = Sequential([
    TCN(input_shape=(sequence_length, 1)),
    Dense(1)
])
tcn_model.compile(optimizer='adam', loss='mean_squared_error')

# Display the model's architecture
tcn_full_summary(tcn_model, expand_residual_blocks=False)

# Train the model
tcn_model.fit(X_train, y_train, epochs=20, batch_size=32)

# Make predictions
tcn_predictions = tcn_model.predict(X_test)
tcn_predictions = scaler.inverse_transform(tcn_predictions.reshape(-1, 1))

# Inverse transform the test target as well
y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

# Calculate evaluation metrics
mae_tcn = mean_absolute_error(y_test, tcn_predictions)
rmse_tcn = np.sqrt(mean_squared_error(y_test, tcn_predictions))
mape_tcn = np.mean(np.abs((y_test - tcn_predictions) / y_test)) * 100

# Print evaluation metrics
print(f'TCN - MAE: {mae_tcn}')
print(f'TCN - RMSE: {rmse_tcn}')
print(f'TCN - MAPE: {mape_tcn:.2f}%')

# Create a DataFrame to plot the results
results = pd.DataFrame({
    'Actual': y_test.flatten(),
    'Predicted': tcn_predictions.flatten()
}, index=data.index[-len(y_test):])

# Plot the results for TCN
fig = plt.figure(figsize=(25, 8))
plt.plot(results.index, results['Actual'], 'r.', label='Test Data')
plt.plot(results.index, results['Predicted'], 'b-', label='Forecast (TCN)')
plt.title('TCN - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Filter the results DataFrame to get the subset for the specific date range
subset_results = results[(results.index >= start_datem) & (results.index <= end_datem)]

# Plot the results for the specific date range
fig = plt.figure(figsize=(25, 8))
plt.plot(subset_results.index, subset_results['Actual'], 'r--', label='Test Data')  # Dotted line for test data
plt.plot(subset_results.index, subset_results['Predicted'], 'b-', label='Forecast (TCN)')  # Solid line for forecast
plt.title('TCN Model - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Filter the results DataFrame to get the subset for the specific date range
subset_results = results[(results.index >= start_date) & (results.index <= end_date)]

# Plot the results for the specific date range
fig = plt.figure(figsize=(25, 8))
plt.plot(subset_results.index, subset_results['Actual'], 'r--', label='Test Data')  # Dotted line for test data
plt.plot(subset_results.index, subset_results['Predicted'], 'b-', label='Forecast (TCN)')  # Solid line for forecast
plt.title('TCN Model - Test Data vs Forecast for November 1-7, 2023')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

"""HYBRID"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Load your data (assuming it's in a DataFrame named 'hourly_demand')
data = hourly_demand[['DATETIME', 'ND']].rename(columns={'DATETIME': 'ds', 'ND': 'y'})

# Ensure that 'ds' column is of datetime type
data['ds'] = pd.to_datetime(data['ds'])

# Set the datetime column as index
data.set_index('ds', inplace=True)

# Create lag features
data['lag_1'] = data['y'].shift(1)
data['lag_2'] = data['y'].shift(2)
data['lag_3'] = data['y'].shift(3)

# Drop missing values
data.dropna(inplace=True)

# Split the data into training and testing sets based on a specific date
train = data[data.index < '2023-01-01']
test = data[data.index >= '2023-01-01']

# Define features and target for both train and test
X_train = train.drop(columns='y')
y_train = train['y']
X_test = test.drop(columns='y')
y_test = test['y']

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten

# Define the TCN model
n_steps = X_train.shape[1]
n_features = 1  # Since each lag feature is treated as a feature

tcn_model = Sequential([
    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_steps, n_features)),
    Flatten(),
    Dense(50, activation='relu'),
    Dense(1)
])

tcn_model.compile(optimizer='adam', loss='mse')

# Reshape data for TCN input
X_train_tcn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], n_features))
X_test_tcn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], n_features))

# Train the TCN model
tcn_model.fit(X_train_tcn, y_train, epochs=20, verbose=1)

# Generate predictions using the TCN model
tcn_predictions = tcn_model.predict(X_test_tcn)

import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Combine TCN predictions with the original test features
X_combined = np.hstack((X_test, tcn_predictions))

# Train the XGBoost model using the combined features
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
xgb_model.fit(X_combined, y_test)

# Generate predictions
xgb_predictions = xgb_model.predict(X_combined)

# Evaluation metrics
mae = mean_absolute_error(y_test, xgb_predictions)
rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))
mape = np.mean(np.abs((y_test - xgb_predictions) / y_test)) * 100

print(f'MAE: {mae:.2f}')
print(f'RMSE: {rmse:.2f}')
print(f'MAPE: {mape:.2f}%')

import matplotlib.pyplot as plt

# Create a DataFrame to plot the results
results_hybrid_tcn_xgb = pd.DataFrame({
    'Actual': y_test.values,
    'Hybrid_Predicted': xgb_predictions.flatten()
}, index=test.index)

# Plot the results for the hybrid model
fig = plt.figure(figsize=(25, 8))
plt.plot(results_hybrid_tcn_xgb.index, results_hybrid_tcn_xgb['Actual'], 'r--', label='Test Data')
plt.plot(results_hybrid_tcn_xgb.index, results_hybrid_tcn_xgb['Hybrid_Predicted'], 'b-', label='Hybrid Forecast (TCN + XGBoost)')
plt.title('Hybrid Model - TCN + XGBoost - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Filter the test set and hybrid predictions for the specific period
test_subset = y_test.loc[start_datem:end_datem]
predicted_subset = results_hybrid_tcn_xgb['Hybrid_Predicted'].loc[start_datem:end_datem]

# Plot the results for the specific period
fig = plt.figure(figsize=(25, 8))
plt.plot(test_subset.index, test_subset, 'r--', label='Test Data')
plt.plot(predicted_subset.index, predicted_subset, 'b-', label='Hybrid Forecast (TCN + XGBoost)')
plt.title('Hybrid Model - TCN + XGBoost - Test Data vs Forecast)')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Define the start and end date for the subset you want to focus on (e.g., July 2023)
start_date = pd.Timestamp('2023-11-01')
end_date = pd.Timestamp('2023-11-07')

# Filter the test set and hybrid predictions for the specific period
test_subset = y_test.loc[start_date:end_date]
predicted_subset = results_hybrid_tcn_xgb['Hybrid_Predicted'].loc[start_date:end_date]

# Plot the results for the specific period
fig = plt.figure(figsize=(25, 8))
plt.plot(test_subset.index, test_subset, 'r--', label='Test Data')
plt.plot(predicted_subset.index, predicted_subset, 'b-', label='Hybrid Forecast (TCN + XGBoost)')
plt.title('Hybrid Model - TCN + XGBoost - Test Data vs Forecast (November 1-7, 2023)')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()







"""COMPARE ALL MODELS"""



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure all predictions are correctly aligned to the same length
max_len = min(len(y_test), len(rf_predictions), len(xgb_predictions), len(lstm_predictions),
              len(tcn_predictions), len(results_hybrid_tcn_xgb))

# Truncate or pad predictions and actuals to the same length and convert to NumPy arrays
y_test_aligned = y_test[:max_len].values.ravel()  # Assuming y_test is a Pandas Series
rf_predictions_aligned = rf_predictions[:max_len].ravel()  # Assuming rf_predictions is a NumPy array
xgb_predictions_aligned = xgb_predictions[:max_len].ravel()  # Assuming xgb_predictions is a NumPy array
lstm_predictions_aligned = lstm_predictions[:max_len].ravel()  # Assuming lstm_predictions is a NumPy array
tcn_predictions_aligned = tcn_predictions[:max_len].ravel()  # Assuming tcn_predictions is a NumPy array
results_hybrid_tcn_xgb_aligned = results_hybrid_tcn_xgb['Hybrid_Predicted'][:max_len].values.ravel()

# Now create the DataFrame
comparison_df = pd.DataFrame({
    'Actual': y_test_aligned,  # Convert Series to NumPy array and flatten
    'Random Forest': rf_predictions_aligned,
    'XGBoost': xgb_predictions_aligned,
    'LSTM': lstm_predictions_aligned,
    'TCN': tcn_predictions_aligned,
    'Hybrid TCN + XGBoost': results_hybrid_tcn_xgb_aligned
}, index=data.index[-max_len:])

# Plot all models' predictions
plt.figure(figsize=(25, 8))
plt.plot(comparison_df.index, comparison_df['Actual'], 'k-', label='Actual')
plt.plot(comparison_df.index, comparison_df['Random Forest'], 'r--', label='Random Forest')
plt.plot(comparison_df.index, comparison_df['XGBoost'], 'g-', label='XGBoost')
plt.plot(comparison_df.index, comparison_df['LSTM'], 'm-', label='LSTM')
plt.plot(comparison_df.index, comparison_df['TCN'], 'c--', label='TCN')
plt.plot(comparison_df.index, comparison_df['Hybrid TCN + XGBoost'], 'y--', label='Hybrid TCN + XGBoost')

plt.title('Model Comparison - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand')
plt.legend()
plt.grid(True)
plt.show()





# Filter the comparison DataFrame for the specific week
df = comparison_df[(comparison_df.index >= start_datem) & (comparison_df.index <= end_datem)]

# Plot the results for the specific week
fig = plt.figure(figsize=(25, 8))
plt.plot(df.index, df['Actual'], 'k-', label='Actual')
plt.plot(df.index, df['Random Forest'], 'r--', label='Random Forest')
plt.plot(df.index, df['XGBoost'], 'g-', label='XGBoost')
plt.plot(df.index, df['LSTM'], 'm-', label='LSTM')
plt.plot(df.index, df['TCN'], 'c--', label='TCN')
plt.plot(df.index, df['Hybrid TCN + XGBoost'], 'y--', label='Hybrid TCN + XGBoost')
plt.title('Model Comparison - Test Data vs Forecast')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()

# Define the start and end date for the specific week (e.g., July 1st to July 7th, 2023)
start_date = pd.Timestamp('2023-07-01')
end_date = pd.Timestamp('2023-07-07')

# Filter the comparison DataFrame for the specific week
week_subset_df = comparison_df[(comparison_df.index >= start_date) & (comparison_df.index <= end_date)]

# Plot the results for the specific week
fig = plt.figure(figsize=(25, 8))
plt.plot(week_subset_df.index, week_subset_df['Actual'], 'k-', label='Actual')
plt.plot(week_subset_df.index, week_subset_df['Random Forest'], 'r--', label='Random Forest')
plt.plot(week_subset_df.index, week_subset_df['XGBoost'], 'g-', label='XGBoost')
plt.plot(week_subset_df.index, week_subset_df['LSTM'], 'm-', label='LSTM')
plt.plot(week_subset_df.index, week_subset_df['TCN'], 'c--', label='TCN')
plt.plot(week_subset_df.index, week_subset_df['Hybrid TCN + XGBoost'], 'y--', label='Hybrid TCN + XGBoost')
plt.title('Model Comparison - Test Data vs Forecast (Week of November 1st, 2023)')
plt.xlabel('Date')
plt.ylabel('Energy Demand(MW)')
plt.legend()
plt.grid(True)
plt.show()



import pandas as pd

# Create a dictionary with model names and their corresponding rounded MAE values
mae_data = {
    'Model': ['Random Forest', 'XGBoost', 'LSTM', 'TCN', 'TCN + XGBoost'],
    'MAE': [302.13, 295.19, 344.11, 323.64, 329.35]
}

# Convert to DataFrame
mae_df = pd.DataFrame(mae_data)

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

# Assuming mae_df is a DataFrame with 'Model' and 'MAE' columns

# Sort the DataFrame in ascending order of MAE (or descending by setting ascending=False)
mae_df_sorted = mae_df.sort_values(by='MAE', ascending=True)

# Define colors for each model
colors = list(mcolors.TABLEAU_COLORS.values())

# Create the figure and axis
plt.figure(figsize=(10, 8))

# Plot horizontal bar chart with no space between them
plt.barh(mae_df_sorted['Model'], mae_df_sorted['MAE'], color=colors[:len(mae_df_sorted)], edgecolor='none')

# Add labels and title
plt.title('MAE Comparison Across Models')
plt.xlabel('Mean Absolute Error (MAE)')
plt.ylabel('Model')
plt.grid(True)

# Remove space between the bars by adjusting the y-axis limits
plt.ylim(-0.5, len(mae_df_sorted) - 0.5)

# Show the plot
plt.show()

import seaborn as sns

# Calculate the absolute errors for each model
errors_df = abs(comparison_df.subtract(comparison_df['Actual'], axis=0))

plt.figure(figsize=(15, 8))
sns.violinplot(data=errors_df)
plt.title('Distribution of Errors for Each Model')
plt.xlabel('Model')
plt.ylabel('Error Distribution')
plt.grid(True)
plt.show()

# Assuming your best model is the TCN model
best_model = best_xgb_model  # Replace this with the actual best model

# Generate predictions with the best model
best_model_predictions = best_model.predict(X_test_scaled)

# Forecast into the future (e.g., next 30 days)
future_steps = 365  # Number of steps to forecast into the future
future_forecast = best_model.predict(X_test_scaled[-future_steps:])  # Adjust this to your model's prediction method

# Plot the forecast
plt.figure(figsize=(25, 8))
plt.plot(data.index[-len(test_target):], test_target, 'r-', label='Actual')
plt.plot(data.index[-len(test_target):], best_model_predictions, 'b-', label='Best Model Prediction')
plt.plot(pd.date_range(start=data.index[-1], periods=future_steps, freq='H'), future_forecast, 'g--', label='Future Forecast')
plt.title('Best Model Forecast into the Future')
plt.xlabel('Date')
plt.ylabel('Energy Demand')
plt.legend()
plt.grid(True)
plt.show()

# Assuming your data has a DateTime index and is hourly
last_week_start = data.index[-1] - pd.Timedelta(days=7)
test_last_week = data[(data.index >= last_week_start) & (data.index <= data.index[-1])]

predictions_last_week = best_model.predict(X_test_scaled[-len(test_last_week):])

# Assuming your data has a DateTime index and is hourly
last_week_start = data.index[-1] - pd.Timedelta(days=7)
test_last_week = data[(data.index >= last_week_start) & (data.index <= data.index[-1])]

# Extract features for the last week (assuming X_test_scaled contains your features)
# Adjust the slicing based on how your features are stored relative to 'data'
X_test_last_week = X_test_scaled[-len(test_last_week):]

# Predictions for the last week
predictions_last_week = best_model.predict(X_test_last_week)

# Forecast for the next 7 days
future_steps = 7 * 24  # 7 days * 24 hours = 168 hours
future_forecast = best_model.predict(X_test_scaled[-future_steps:])

plt.figure(figsize=(25, 8))

# Plot the actual values for the last week of test data
plt.plot(test_last_week.index, test_last_week['y'], 'r-', label='Actual')

# Plot the predictions for the last week of test data
plt.plot(test_last_week.index, predictions_last_week, 'b-', label='Best Model Prediction')

# Plot the forecast for the next 7 days
future_dates = pd.date_range(start=test_last_week.index[-1] + pd.Timedelta(hours=1), periods=future_steps, freq='H')
plt.plot(future_dates, future_forecast, 'g--', label='Future Forecast')

plt.title('Best Model Forecast - Last Week of Test Data and Next 7 Days')
plt.xlabel('Date')
plt.ylabel('Energy Demand')
plt.legend()
plt.grid(True)
plt.show()

# Forecast for the next 30 days (assuming hourly data)
future_steps = 365 * 24  # 365 days * 24 hours = 8760 hours
future_forecast = best_model.predict(X_test_scaled[-future_steps:])

# Generate a date range for the next 30 days
future_dates = pd.date_range(start=data.index[-1] + pd.Timedelta(hours=1), periods=future_steps, freq='H')

# Create a DataFrame to store the forecasted values
forecast_df = pd.DataFrame({
    'Date': future_dates,
    'Forecasted Demand': future_forecast.flatten()  # Flatten the array to 1D
})

# Display the DataFrame
print(forecast_df.head(30))  # Displaying the first 30 rows

# Save the forecast DataFrame to an Excel file
forecast_df.to_excel('forecasted_demand.xlsx', index=False)



# Check which one is longer and drop the first element to align
if len(test_target) > len(xgb_predictions):
    test_target = test_target[-len(xgb_predictions):]
elif len(xgb_predictions) > len(test_target):
    xgb_predictions = xgb_predictions[-len(test_target):]

# Now calculate residuals
residuals = test_target - xgb_predictions

# Align the index with the residuals
aligned_index = test_features.index[-len(residuals):]

# Now plot with the aligned index
plt.figure(figsize=(15, 6))
plt.plot(aligned_index, residuals, 'o', markersize=5)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residuals Over Time')
plt.xlabel('Date')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

# Define the subset time range
subset_start_date = '2023-09-01'
subset_end_date = '2023-12-31'

# Create the subset mask based on the aligned test_target index
subset_mask = (test_target.index >= subset_start_date) & (test_target.index <= subset_end_date)

# Subset the data using the correct mask
subset_test_features = aligned_index[subset_mask]
subset_test_target = test_target[subset_mask]
subset_predictions = xgb_predictions[subset_mask]

# Verify lengths again after subsetting
print(f"Length of subset_test_features: {len(subset_test_features)}")
print(f"Length of subset_test_target: {len(subset_test_target)}")
print(f"Length of subset_predictions: {len(subset_predictions)}")

# Calculate residuals for the subset
subset_residuals = subset_test_target - subset_predictions

# Plot the residuals for the subset
plt.figure(figsize=(15, 6))
plt.plot(subset_test_target.index, subset_residuals, 'o', markersize=5)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Subset Residuals Over Time')
plt.xlabel('Date')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

import scipy.stats as stats

plt.figure(figsize=(10, 6))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('QQ Plot of Residuals')
plt.grid(True)
plt.show()

